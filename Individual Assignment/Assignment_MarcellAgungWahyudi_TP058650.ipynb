{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded63db5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\marce\\appdata\\roaming\\python\\python39\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in e:\\program files\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in e:\\program files\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in e:\\program files\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\program files\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in e:\\program files\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in e:\\program files\\anaconda3\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: jinja2 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (3.0.11)\n",
      "Requirement already satisfied: setuptools in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\program files\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in e:\\program files\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\program files\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\program files\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\program files\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\program files\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in e:\\program files\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\program files\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: colorama in e:\\program files\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in e:\\program files\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\program files\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "     -------------------------------------- 12.8/12.8 MB 537.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in e:\\program files\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.5)\n",
      "Requirement already satisfied: jinja2 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: setuptools in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (63.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.11)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in e:\\program files\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\program files\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in e:\\program files\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\program files\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\program files\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\program files\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\program files\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.14)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\program files\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in e:\\program files\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: colorama in e:\\program files\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in e:\\program files\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\program files\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
      "[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full\n",
      "pipeline package name 'en_core_web_sm' instead.\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#Installing NLTK Library\n",
    "\n",
    "#Every Library needed for everything in this notebook (Run Once)\n",
    "\n",
    "!pip install --user -U nltk\n",
    "!pip install wordcloud\n",
    "!pip install spacy\n",
    "!python -m spacy download en\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13aeb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_txt = open('Data_1.txt', 'r') #Opening txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd13b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1_txt.read() #Making the txt file program readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b1df9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis is \"contextual mining of text which identifies and extracts subjective information\" in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. However, analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics. This is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered. So what should a brand do to capture that low hanging fruit?\n"
     ]
    }
   ],
   "source": [
    "print(data1) #Testing if txt file can be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c567f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x2300e92e140>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer') #Component for sentencizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "350ae4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. Sentence Segmentation\n",
    "doc = nlp(data1)\n",
    "\n",
    "sentence_list = []\n",
    "for sentence in doc.sents:\n",
    "    sentence_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dba4114a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, Sentiment analysis is \"contextual mining of text which identifies and extracts subjective information\" in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations.)\n",
      "(1, However, analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics.)\n",
      "(2, This is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered.)\n",
      "(3, So what should a brand do to capture that low hanging fruit?)\n"
     ]
    }
   ],
   "source": [
    "#Q1 Output\n",
    "\n",
    "for i in enumerate(sentence_list):   #Enmuerate for easy numbering, basic python code\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "880df932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Function Tokenization\n",
      "['Sentiment', 'analysis', 'is', '\"contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information\"', 'in', 'source', 'material,', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand,', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations.', 'However,', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics.', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit?']\n"
     ]
    }
   ],
   "source": [
    "#Split Function\n",
    "print(\"Split Function Tokenization\")\n",
    "splitTokenization = data1.split(' ')\n",
    "print(splitTokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46f0efaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Expression Tokenizatoin\n",
      "['', 'S', 'e', 'n', 't', 'i', 'm', 'e', 'n', 't', ' ', 'a', 'n', 'a', 'l', 'y', 's', 'i', 's', ' ', 'i', 's', ' ', '\"', 'c', 'o', 'n', 't', 'e', 'x', 't', 'u', 'a', 'l', ' ', 'm', 'i', 'n', 'i', 'n', 'g', ' ', 'o', 'f', ' ', 't', 'e', 'x', 't', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 'd', 'e', 'n', 't', 'i', 'f', 'i', 'e', 's', ' ', 'a', 'n', 'd', ' ', 'e', 'x', 't', 'r', 'a', 'c', 't', 's', ' ', 's', 'u', 'b', 'j', 'e', 'c', 't', 'i', 'v', 'e', ' ', 'i', 'n', 'f', 'o', 'r', 'm', 'a', 't', 'i', 'o', 'n', '\"', ' ', 'i', 'n', ' ', 's', 'o', 'u', 'r', 'c', 'e', ' ', 'm', 'a', 't', 'e', 'r', 'i', 'a', 'l', ',', ' ', 'a', 'n', 'd', ' ', 'h', 'e', 'l', 'p', 'i', 'n', 'g', ' ', 'a', ' ', 'b', 'u', 's', 'i', 'n', 'e', 's', 's', ' ', 't', 'o', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 's', 'o', 'c', 'i', 'a', 'l', ' ', 's', 'e', 'n', 't', 'i', 'm', 'e', 'n', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'b', 'r', 'a', 'n', 'd', ',', ' ', 'p', 'r', 'o', 'd', 'u', 'c', 't', ' ', 'o', 'r', ' ', 's', 'e', 'r', 'v', 'i', 'c', 'e', ' ', 'w', 'h', 'i', 'l', 'e', ' ', 'm', 'o', 'n', 'i', 't', 'o', 'r', 'i', 'n', 'g', ' ', 'o', 'n', 'l', 'i', 'n', 'e', ' ', 'c', 'o', 'n', 'v', 'e', 'r', 's', 'a', 't', 'i', 'o', 'n', 's', '.', ' ', 'H', 'o', 'w', 'e', 'v', 'e', 'r', ',', ' ', 'a', 'n', 'a', 'l', 'y', 's', 'i', 's', ' ', 'o', 'f', ' ', 's', 'o', 'c', 'i', 'a', 'l', ' ', 'm', 'e', 'd', 'i', 'a', ' ', 's', 't', 'r', 'e', 'a', 'm', 's', ' ', 'i', 's', ' ', 'u', 's', 'u', 'a', 'l', 'l', 'y', ' ', 'r', 'e', 's', 't', 'r', 'i', 'c', 't', 'e', 'd', ' ', 't', 'o', ' ', 'j', 'u', 's', 't', ' ', 'b', 'a', 's', 'i', 'c', ' ', 's', 'e', 'n', 't', 'i', 'm', 'e', 'n', 't', ' ', 'a', 'n', 'a', 'l', 'y', 's', 'i', 's', ' ', 'a', 'n', 'd', ' ', 'c', 'o', 'u', 'n', 't', ' ', 'b', 'a', 's', 'e', 'd', ' ', 'm', 'e', 't', 'r', 'i', 'c', 's', '.', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'k', 'i', 'n', ' ', 't', 'o', ' ', 'j', 'u', 's', 't', ' ', 's', 'c', 'r', 'a', 't', 'c', 'h', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 's', 'u', 'r', 'f', 'a', 'c', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'i', 's', 's', 'i', 'n', 'g', ' ', 'o', 'u', 't', ' ', 'o', 'n', ' ', 't', 'h', 'o', 's', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'v', 'a', 'l', 'u', 'e', ' ', 'i', 'n', 's', 'i', 'g', 'h', 't', 's', ' ', 't', 'h', 'a', 't', ' ', 'a', 'r', 'e', ' ', 'w', 'a', 'i', 't', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'b', 'e', ' ', 'd', 'i', 's', 'c', 'o', 'v', 'e', 'r', 'e', 'd', '.', ' ', 'S', 'o', ' ', 'w', 'h', 'a', 't', ' ', 's', 'h', 'o', 'u', 'l', 'd', ' ', 'a', ' ', 'b', 'r', 'a', 'n', 'd', ' ', 'd', 'o', ' ', 't', 'o', ' ', 'c', 'a', 'p', 't', 'u', 'r', 'e', ' ', 't', 'h', 'a', 't', ' ', 'l', 'o', 'w', ' ', 'h', 'a', 'n', 'g', 'i', 'n', 'g', ' ', 'f', 'r', 'u', 'i', 't', '?', '']\n"
     ]
    }
   ],
   "source": [
    "#RegEx\n",
    "print(\"Regular Expression Tokenizatoin\")\n",
    "regExTokenization = re.split('', data1)\n",
    "print(regExTokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8cf94ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokenization\n",
      "['Sentiment', 'analysis', 'is', '``', 'contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information', \"''\", 'in', 'source', 'material', ',', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations', '.', 'However', ',', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics', '.', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered', '.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit', '?']\n"
     ]
    }
   ],
   "source": [
    "#NLTK Tokenization\n",
    "print(\"NLTK Tokenization\")\n",
    "nltkTokenization = nltk.tokenize.word_tokenize(data1)\n",
    "print(nltkTokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9e12b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Senti', 'analysi', 'i', '``', 'contextual', 'min', 'of', 'text', 'which', 'identif', 'and', 'extract', 'subject', 'information', \"''\", 'in', 'source', 'material', ',', 'and', 'help', 'a', 'busines', 'to', 'understand', 'the', 'social', 'senti', 'of', 'their', 'brand', ',', 'product', 'or', 'service', 'while', 'monitor', 'online', 'conversation', '.', 'However', ',', 'analysi', 'of', 'social', 'media', 'stream', 'i', 'usual', 'restrict', 'to', 'just', 'basic', 'senti', 'analysi', 'and', 'count', 'bas', 'metric', '.', 'Thi', 'i', 'akin', 'to', 'just', 'scratch', 'the', 'surface', 'and', 'miss', 'out', 'on', 'those', 'high', 'value', 'insight', 'that', 'are', 'wait', 'to', 'be', 'discover', '.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hang', 'fruit', '?']\n"
     ]
    }
   ],
   "source": [
    "def stem(words):\n",
    "    regexp = r'^(.+?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, words)[0]\n",
    "    return stem\n",
    "\n",
    "tokens = word_tokenize(data1)\n",
    "print([stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e4aeef18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'analysi', 'is', '``', 'contextu', 'mine', 'of', 'text', 'which', 'identifi', 'and', 'extract', 'subject', 'inform', \"''\", 'in', 'sourc', 'materi', ',', 'and', 'help', 'a', 'busi', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'servic', 'while', 'monitor', 'onlin', 'convers', '.', 'howev', ',', 'analysi', 'of', 'social', 'media', 'stream', 'is', 'usual', 'restrict', 'to', 'just', 'basic', 'sentiment', 'analysi', 'and', 'count', 'base', 'metric', '.', 'thi', 'is', 'akin', 'to', 'just', 'scratch', 'the', 'surfac', 'and', 'miss', 'out', 'on', 'those', 'high', 'valu', 'insight', 'that', 'are', 'wait', 'to', 'be', 'discov', '.', 'so', 'what', 'should', 'a', 'brand', 'do', 'to', 'captur', 'that', 'low', 'hang', 'fruit', '?']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "wr = data1.split() \n",
    "wr = [s for s in wr if s != '<s>']\n",
    "wr = [s for s in wr if s != '</s>']\n",
    "lst = []\n",
    "\n",
    "for i in wr :\n",
    "    if i not in lst :\n",
    "        lst.append(i) #Unique values in text appended into list \n",
    "        \n",
    "words = word_tokenize(data1)\n",
    "print([ps.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7bc4e83",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.2.2-cp39-cp39-win_amd64.whl (153 kB)\n",
      "     -------------------------------------- 153.1/153.1 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow in e:\\program files\\anaconda3\\lib\\site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in e:\\program files\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in e:\\program files\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\program files\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\program files\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\program files\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\program files\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in e:\\program files\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in e:\\program files\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in e:\\program files\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.2.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f7e033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3. Filter Stop Words and Punctuation\n",
    "import nltk, string\n",
    "import wordcloud\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b07ed6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'analysis', 'is', '``', 'contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information', \"''\", 'in', 'source', 'material', ',', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations', '.', 'however', ',', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics', '.', 'this', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered', '.', 'so', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit', '?']\n",
      "Total Words in Text:  96\n",
      "\n",
      "Total Words after removing stop words:  51\n",
      "['sentiment', 'analysis', '``', 'contextual', 'mining', 'text', 'identifies', 'extracts', 'subjective', 'information', \"''\", 'source', 'material', 'helping', 'business', 'understand', 'social', 'sentiment', 'brand', 'product', 'service', 'monitoring', 'online', 'conversations', 'however', 'analysis', 'social', 'media', 'streams', 'usually', 'restricted', 'basic', 'sentiment', 'analysis', 'count', 'based', 'metrics', 'akin', 'scratching', 'surface', 'missing', 'high', 'value', 'insights', 'waiting', 'discovered', 'brand', 'capture', 'low', 'hanging', 'fruit']\n",
      "\n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "textLower = data1.lower() #lower() returns all lower case characters \n",
    "wordTokens = nltk.tokenize.word_tokenize(textLower)\n",
    "print(wordTokens) #check the previous tokens before filtering\n",
    "\n",
    "print(\"Total Words in Text: \",len(wordTokens))\n",
    "\n",
    "stopTokens = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)\n",
    "filteredTokens = []\n",
    "\n",
    "for word in wordTokens: #Starts filtering stop words\n",
    "    if word not in stopTokens:\n",
    "        filteredTokens.append(word)\n",
    "        \n",
    "print(\"\\nTotal Words after removing stop words: \", len(filteredTokens))\n",
    "print(filteredTokens)\n",
    "print(\"\\n\", stopTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87a45b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = open('Data_2.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68d6b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'videogame', 'or', 'computergame', 'is', 'an', 'electronic-game', 'that', 'involves', 'interaction', 'with', 'a', 'user', 'interface', 'or', 'input', 'device']\n",
      "\n",
      "\n",
      "[('A', 'DT'), ('videogame', 'NN'), ('or', 'CC'), ('computergame', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('electronic-game', 'JJ'), ('that', 'WDT'), ('involves', 'VBZ'), ('interaction', 'NN'), ('with', 'IN'), ('a', 'DT'), ('user', 'JJ'), ('interface', 'NN'), ('or', 'CC'), ('input', 'NN'), ('device', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Q4 Pos Tagging using NLTK, RegEx\n",
    "#NLTK\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "wordTokens = word_tokenize(data2)\n",
    "print(tokens)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "nltkTag = nltk.pos_tag(wordTokens)\n",
    "print(nltkTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74c54ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After extracting \n",
      " (S\n",
      "  (NP A/DT videogame/NN)\n",
      "  or/CC\n",
      "  (NP computergame/NN)\n",
      "  (VP (V is/VBZ))\n",
      "  an/DT\n",
      "  electronic-game/JJ\n",
      "  that/WDT\n",
      "  (VP (V involves/VBZ) (NP interaction/NN))\n",
      "  (P with/IN)\n",
      "  (NP a/DT user/JJ interface/NN)\n",
      "  or/CC\n",
      "  (NP input/NN)\n",
      "  (NP device/NN))\n"
     ]
    }
   ],
   "source": [
    "#RegEx\n",
    "\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "\n",
    "tag = pos_tag(word_tokenize(data2))\n",
    "\n",
    "chunker = RegexpParser(\"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>} #To extract Noun Phrases\n",
    "P: {<IN>}            #To extract Prepositions\n",
    "V: {<V.*>}           #To extract Verbs\n",
    "PP: {<p> <NP>}       #To extract Prepositional Phrases\n",
    "VP: {<V> <NP|PP>*}   #To extract Verb Phrases\n",
    "                    \"\"\")\n",
    "\n",
    "regEx = chunker.parse(tag)\n",
    "print(\"After extracting \\n\", regEx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df278b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\marce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "regEx.draw() #Draws plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
