{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3e4ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.probability import FreqDist, LidstoneProbDist, ConditionalFreqDist, ConditionalProbDist\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import copy\n",
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "# from nltk.lm import MLEProbDist, LaplaceProbDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c39244db",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Text Corpus.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4120\\3994137911.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Importing Text File\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Text Corpus.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Text Corpus.txt'"
     ]
    }
   ],
   "source": [
    "#Importing Text File\n",
    "with open('Text Corpus.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ff947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Lowercases all words and removes non-alphanumeric characters.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908f2209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c12bcef",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8646dd1",
   "metadata": {},
   "source": [
    "# 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77db9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsmoothed Unigram Language Model Probability: \n",
      "P(read): 0.14285714285714285\n",
      "P(He): 0.09523809523809523\n",
      "P(I): 0.047619047619047616\n",
      "P(my): 0.047619047619047616\n",
      "P(different): 0.047619047619047616\n",
      "P(book): 0.14285714285714285\n",
      "P(a): 0.14285714285714285\n",
      "P(Danielle): 0.047619047619047616\n"
     ]
    }
   ],
   "source": [
    "#1.1 Unsmoothed Unigram Language Model\n",
    "def train_unigram(text):\n",
    "    \"\"\"\n",
    "    Unigram Training, Will riturn Dictionary with word freqs\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    frequencies = nltk.FreqDist(words)\n",
    "    return frequencies\n",
    "\n",
    "def predict_unigram(text, model):\n",
    "    \"\"\"\n",
    "    Using the trained unigram (Dictionary) will predict the probability\n",
    "    WIll return float\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    probability = 1.0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            probability *= model.freq(word)\n",
    "        else:\n",
    "            probability *= 0.0\n",
    "    return probability\n",
    "\n",
    "# Output\n",
    "\n",
    "model = train_unigram(text)\n",
    "\n",
    "words = text.split(\" \")\n",
    "words.remove(\"<s>\")\n",
    "words.remove(\"</s>\")\n",
    "words.remove(\"</s>\\n<s>\")\n",
    "words.remove(\"</s>\\n<s>\")\n",
    "\n",
    "uniqueWords = set(words)\n",
    "\n",
    "print(\"Unsmoothed Unigram Language Model Probability: \")\n",
    "for word in uniqueWords:\n",
    "    probability = predict_unigram(word, model)\n",
    "    print('P('+word+'):', probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd6dd95",
   "metadata": {},
   "source": [
    "# 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f7254d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsmoothed Unigram Language Model Probability: \n",
      "P(a): 0.14155251141552513\n",
      "P(book): 0.14155251141552513\n",
      "P(I): 0.05022831050228311\n",
      "P(different): 0.05022831050228311\n",
      "P(my): 0.05022831050228311\n",
      "P(read): 0.14155251141552513\n",
      "P(Danielle): 0.05022831050228311\n",
      "P(He): 0.09589041095890412\n"
     ]
    }
   ],
   "source": [
    "#1.2 Smoothed Unigram Language Model\n",
    "def train_unigram(text, gamma=0.1):\n",
    "    \"\"\"\n",
    "    Unigram Training, Will riturn Dictionary with word freqs\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    n = len(words)\n",
    "    frequencies = FreqDist(words)\n",
    "    vocabulary_size = len(frequencies)\n",
    "    model = {}\n",
    "    for word, freq in frequencies.items():\n",
    "        model[word] = LidstoneProbDist(frequencies, gamma, vocabulary_size)\n",
    "    return model\n",
    "\n",
    "def predict_unigram(text, model):\n",
    "    \"\"\"\n",
    "    Using the trained unigram (Dictionary) will predict the probability\n",
    "    WIll return float\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    probability = 1.0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            probability *= model[word].prob(word)\n",
    "        else:\n",
    "            probability *= model[word].prob('<UNK>')\n",
    "    return probability\n",
    "\n",
    "\n",
    "# Output\n",
    "model = train_unigram(text, gamma=0.1)\n",
    "\n",
    "words = text.split(\" \")\n",
    "words.remove(\"<s>\")\n",
    "words.remove(\"</s>\")\n",
    "words.remove(\"</s>\\n<s>\")\n",
    "words.remove(\"</s>\\n<s>\")\n",
    "\n",
    "uniqueWords = set(words)\n",
    "\n",
    "print(\"Unsmoothed Unigram Language Model Probability: \")\n",
    "for word in uniqueWords:\n",
    "    probability = predict_unigram(word, model)\n",
    "    print('P('+word+'):', probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f443502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b676f0f",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7faba6",
   "metadata": {},
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ff2acb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Probability     Word 1     Word 2\n",
      "(s, he)               0.333333          s         he\n",
      "(he, read)            1.000000         he       read\n",
      "(read, a)             1.000000       read          a\n",
      "(a, book)             0.666667          a       book\n",
      "(book, s)             0.666667       book          s\n",
      "(s, s)                0.333333          s          s\n",
      "(s, i)                0.166667          s          i\n",
      "(i, read)             1.000000          i       read\n",
      "(a, different)        0.333333          a  different\n",
      "(different, book)     1.000000  different       book\n",
      "(book, my)            0.333333       book         my\n",
      "(my, danielle)        1.000000         my   danielle\n",
      "(danielle, s)         1.000000   danielle          s\n"
     ]
    }
   ],
   "source": [
    "#2.1 Unsmoothed Bigram Language Model\n",
    "#PreProcessing\n",
    "text = text.lower()\n",
    "text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "sentences = nltk.word_tokenize(text)\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams = list(ngrams(sentences, 2))\n",
    "\n",
    "# Count frequency of bigram\n",
    "frequencies = Counter(bigrams)\n",
    "\n",
    "# Calculate probabilities\n",
    "probabilities = {}\n",
    "for bigram, frequency in frequencies.items():\n",
    "    first_word = bigram[0]\n",
    "    probability = frequency / sentences.count(first_word)\n",
    "    probabilities[bigram] = probability\n",
    "\n",
    "# Print the bigram probabilities\n",
    "df = pd.DataFrame.from_dict(probabilities, orient='index', columns=['Probability'])\n",
    "\n",
    "# Add separate columns for the first and second words of the bigram\n",
    "df[['Word 1', 'Word 2']] = pd.DataFrame(df.index.tolist(), index=df.index)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8312ef",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fde2ec4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nThe number of bins in a Lidstone distribution (9) must be greater than or equal to\nthe number of bins in the FreqDist used to create it (14).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29088\\1017376770.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Define the smoothed probability distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0msmoothed_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLidstoneProbDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Calculate the probabilities of each bigram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, freqdist, gamma, bins)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    810\u001b[0m                 \u001b[1;34m'\\nThe number of bins in a %s distribution '\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[1;33m+\u001b[0m \u001b[1;34m'(%d) must be greater than or equal to\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: \nThe number of bins in a Lidstone distribution (9) must be greater than or equal to\nthe number of bins in the FreqDist used to create it (14)."
     ]
    }
   ],
   "source": [
    "#Q2.2 Smoothed Bigram Languange Model\n",
    "#PreProcessing\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import LidstoneProbDist, FreqDist\n",
    "\n",
    "sentences = [\n",
    "    ['s', 'he', 'read', 'a', 'book', 's'],\n",
    "    ['i', 'read', 'a', 'different', 'book', 's'],\n",
    "    ['s', 'he', 'read', 'a', 'book', 'my', 'danielle', 's']\n",
    "]\n",
    "\n",
    "# Collect all bigrams from the sentences\n",
    "all_bigrams = []\n",
    "for sent in sentences:\n",
    "    bigrams = ngrams(sent, 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "    all_bigrams += list(bigrams)\n",
    "\n",
    "# Calculate the frequency distribution of bigrams\n",
    "bigram_freq = FreqDist(all_bigrams)\n",
    "\n",
    "# Calculate the vocabulary size\n",
    "vocab_size = len(set([word for sent in sentences for word in sent]))\n",
    "\n",
    "# Define the smoothing parameter\n",
    "gamma = 0.5\n",
    "\n",
    "# Define the smoothed probability distribution\n",
    "smoothed_prob = LidstoneProbDist(bigram_freq, gamma, vocab_size)\n",
    "\n",
    "# Calculate the probabilities of each bigram\n",
    "bigram_prob = {}\n",
    "for bigram in bigram_freq:\n",
    "    prob = smoothed_prob.prob(bigram[1], bigram[0])\n",
    "    bigram_prob[bigram] = prob\n",
    "\n",
    "# Print the probabilities of each bigram\n",
    "for bigram, prob in bigram_prob.items():\n",
    "    print(f'{bigram}: {prob}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6085c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "774ab0e2",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "548db1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram and Bigram Language Model Sentence Probability: \n",
      "Sentence: s he read a book s\n",
      "Unigram probability: 2.266629267283756e-05\n",
      "Bigram probability: 1.3751208137469954e-05\n",
      "\n",
      "Sentence: s i read a different book s\n",
      "Unigram probability: 5.396736350675608e-07\n",
      "Bigram probability: 1.0206435810159401e-07\n",
      "\n",
      "Sentence: s he read a book my danielle s\n",
      "Unigram probability: 5.139748905405342e-08\n",
      "Bigram probability: 1.9210577005037543e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q3 Sentence Probability\n",
    "\n",
    "def train_unigram(text, gamma=0.1):\n",
    "    \"\"\"\n",
    "    Trains an unsmoothed unigram language model on the input text.\n",
    "    gamma is the smoothing parameter and defaults to 0.1.\n",
    "    Returns a dictionary with unigram probabilities.\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    frequencies = FreqDist(words)\n",
    "    total = len(words)\n",
    "    vocab_size = len(set(words))\n",
    "    model = {}\n",
    "    for word in frequencies:\n",
    "        model[word] = frequencies[word]/total\n",
    "    return model\n",
    "\n",
    "def train_bigram(text, gamma=0.1):\n",
    "    \"\"\"\n",
    "    Trains a smoothed bigram language model on the input text.\n",
    "    gamma is the smoothing parameter and defaults to 0.1.\n",
    "    Returns a dictionary with bigram probabilities.\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    bigrams = list(ngrams(words, 2))\n",
    "    frequencies = FreqDist(bigrams)\n",
    "    total = len(bigrams)\n",
    "    vocab_size = len(set(words))\n",
    "    model = {}\n",
    "    for bigram in frequencies:\n",
    "        model[bigram] = LidstoneProbDist(frequencies, gamma).prob(bigram) \n",
    "    return model\n",
    "\n",
    "def sentence_probability_unigram(sentence, model):\n",
    "    \"\"\"\n",
    "    Calculates the probability of the input sentence using the trained unigram model.\n",
    "    Returns the probability as a float.\n",
    "    \"\"\"\n",
    "    words = preprocess_text(sentence)\n",
    "    probability = 1.0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            probability *= model[word]\n",
    "        else:\n",
    "            probability *= 0.0\n",
    "    return probability\n",
    "\n",
    "def sentence_probability_bigram(sentence, model):\n",
    "    \"\"\"\n",
    "    Calculates the probability of the input sentence using the trained bigram model.\n",
    "    Returns the probability as a float.\n",
    "    \"\"\"\n",
    "    words = preprocess_text(sentence)\n",
    "    bigrams = list(ngrams(words, 2))\n",
    "    probability = 1.0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in model:\n",
    "            probability *= model[bigram]\n",
    "        else:\n",
    "            probability *= 0.0\n",
    "    return probability\n",
    "\n",
    "# Output\n",
    "\n",
    "unigram_model = train_unigram(text)\n",
    "bigram_model = train_bigram(text)\n",
    "\n",
    "sentences = text.split(\"\\n\")\n",
    "\n",
    "uniqueWords = set(words)\n",
    "\n",
    "print(\"Unigram and Bigram Language Model Sentence Probability: \")\n",
    "for sentence in sentences:\n",
    "    unigram_probability = sentence_probability_unigram(sentence, unigram_model)\n",
    "    bigram_probability = sentence_probability_bigram(sentence, bigram_model)\n",
    "    print(f\"Sentence: {sentence}\\nUnigram probability: {unigram_probability}\\nBigram probability: {bigram_probability}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e808ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Probability     Word 1     Word 2\n",
      "(s, he)               0.333333          s         he\n",
      "(he, read)            1.000000         he       read\n",
      "(read, a)             1.000000       read          a\n",
      "(a, book)             0.666667          a       book\n",
      "(book, s)             0.666667       book          s\n",
      "(s, s)                0.333333          s          s\n",
      "(s, i)                0.166667          s          i\n",
      "(i, read)             1.000000          i       read\n",
      "(a, different)        0.333333          a  different\n",
      "(different, book)     1.000000  different       book\n",
      "(book, my)            0.333333       book         my\n",
      "(my, danielle)        1.000000         my   danielle\n",
      "(danielle, s)         1.000000   danielle          s\n"
     ]
    }
   ],
   "source": [
    "#PreProcessing\n",
    "text = text.lower()\n",
    "text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "sentences = nltk.word_tokenize(text)\n",
    "\n",
    "# Generate bigrams from the input sentence\n",
    "bigrams = list(ngrams(sentences, 2))\n",
    "\n",
    "# Count the frequency of each bigram\n",
    "frequencies = Counter(bigrams)\n",
    "\n",
    "# Calculate the bigram probabilities\n",
    "probabilities = {}\n",
    "for bigram, frequency in frequencies.items():\n",
    "    first_word = bigram[0]\n",
    "    probability = frequency / sentences.count(first_word)\n",
    "    probabilities[bigram] = probability\n",
    "\n",
    "# Print the bigram probabilities\n",
    "df = pd.DataFrame.from_dict(probabilities, orient='index', columns=['Probability'])\n",
    "\n",
    "# Add separate columns for the first and second words of the bigram\n",
    "df[['Word 1', 'Word 2']] = pd.DataFrame(df.index.tolist(), index=df.index)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6318790",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4d7ce9",
   "metadata": {},
   "source": [
    "# 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "795c9f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Owen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae719974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data set\n",
    "path = (\"Musical_Instruments_Reviews.csv\")\n",
    "data = pd.read_csv(path)\n",
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a8c956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labeling prediced sentiments\n",
    "for index, row in df.iterrows():\n",
    "  sent=sia.polarity_scores(row['Reviews'])\n",
    "  if (sent['compound'] == 0):\n",
    "      sentiment=\"Neutral\"\n",
    "  else:\n",
    "      if (sent['compound'] > -0):\n",
    "            sentiment=\"Positive\"\n",
    "      else:\n",
    "            sentiment=\"Negative\"\n",
    "  df.at[index,'Sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc1225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export predicted resulting data set\n",
    "df.to_csv(\"Musical_Instruments_Reviews_Result.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f56b7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.085, 'neu': 0.727, 'pos': 0.188, 'compound': 0.905}\n"
     ]
    }
   ],
   "source": [
    "print(sia.polarity_scores(row['Reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2b61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c003da87",
   "metadata": {},
   "source": [
    "# 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5e82e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Owen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b32579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data set from Q4.1\n",
    "path = (\"Musical_Instruments_Reviews_Result.csv\")\n",
    "newdata = pd.read_csv(path)\n",
    "df=pd.DataFrame(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c1d545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9070\n",
      "996\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "#Amount of reviews\n",
    "\n",
    "#Number of Positive\n",
    "print(len(df[df['Sentiment']=='Positive']))\n",
    "\n",
    "#Number of Negative\n",
    "print(len(df[df['Sentiment']=='Negative']))\n",
    "\n",
    "#Number of Neutral\n",
    "print(len(df[df['Sentiment']=='Neutral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba500aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data pre processing\n",
    "\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "#create word tokenizer features for Positive, negative, and neutral reviews\n",
    "neg_data = df[df['Sentiment'] == 'Negative']\n",
    "pos_data = df[df['Sentiment'] == 'Positive']\n",
    "neut_data=df[df['Sentiment'] == 'Neutral']\n",
    "pos_feats = [(word_feats(word_tokenize(review)), 'pos') for review in pos_data['Reviews']]\n",
    "neg_feats = [(word_feats(word_tokenize(review)), 'neg') for review in neg_data['Reviews']]\n",
    "neut_feats = [(word_feats(word_tokenize(review)), 'neut') for review in neut_data['Reviews']]\n",
    "\n",
    "#Creating training and test data set\n",
    "negcutoff = int(len(neg_feats) * 3 / 4)\n",
    "poscutoff = int(len(pos_feats) * 3 / 4)\n",
    "neutcutoff = int(len(neut_feats) * 3 / 4)\n",
    "trainfeats = neg_feats[:negcutoff] + pos_feats[:poscutoff]+neut_feats[:neutcutoff]\n",
    "testfeats = neg_feats[negcutoff:] + pos_feats[poscutoff:]+neut_feats[neutcutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b30c376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating NaiveBayes Model\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(testfeats):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3b6809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy:  0.8794851794071763\n",
      "\n",
      "Positive Precision:  0.8933012434817489\n",
      "Positive recall: 0.9819223985890653\n",
      "Positive F1 Score: 0.9355177483721908\n"
     ]
    }
   ],
   "source": [
    "#Performance Measures\n",
    "\n",
    "#Accuracy\n",
    "print(\"\\nModel Accuracy: \",nltk.classify.accuracy(classifier, testfeats))\n",
    "\n",
    "#Precision, Recall, F1\n",
    "print (\"\\nPositive Precision: \",nltk.scores.precision(refsets['pos'], testsets['pos']))\n",
    "print ('Positive recall:', nltk.scores.recall(refsets['pos'], testsets['pos']))\n",
    "print ('Positive F1 Score:', nltk.scores.f_measure(refsets['pos'], testsets['pos']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
